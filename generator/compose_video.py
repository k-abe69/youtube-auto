import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import random

import json
import numpy as np
import re
import math

from moviepy.editor import *
from moviepy.video.fx.all import fadein
from common.constants import SILENCE_DURATION
from pathlib import Path
from dotenv import load_dotenv
from common.backup_script import backup_script
from common.save_config import save_config_snapshot
from common.script_utils import get_next_script_id, mark_script_completed
from PIL import Image
from common.video_export import export_video_high_quality

from datetime import datetime, timedelta
from collections import defaultdict


# åˆæœŸå‡¦ç†
backup_script(__file__)
save_config_snapshot()
load_dotenv()

# å®šæ•°
VIDEO_WIDTH = 720
VIDEO_HEIGHT = 1280

# def add_overlay_bars_to_final(video_clip, top_height=100, color=(0, 0, 0)):
#     duration = video_clip.duration

#     top_bar = ColorClip(size=(VIDEO_WIDTH, top_height), color=color, duration=duration)

#     top_bar = top_bar.set_position(("center", 0))

#     return CompositeVideoClip([video_clip, top_bar]).set_audio(video_clip.audio)

def parse_srt_time(t: str) -> float:
    dt = datetime.strptime(t.strip(), "%H:%M:%S,%f")
    return timedelta(
        hours=dt.hour,
        minutes=dt.minute,
        seconds=dt.second,
        microseconds=dt.microsecond
    ).total_seconds()

def check_srt_overlaps(srt_path: Path):
    print("\nğŸ§ å­—å¹•ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãƒã‚§ãƒƒã‚¯é–‹å§‹")
    with open(srt_path, "r", encoding="utf-8") as f:
        content = f.read()

    entries = re.findall(r"(\d+)\n(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})\n", content)

    prev_end = 0
    for idx, (_, start_str, end_str) in enumerate(entries):
        start_sec = parse_srt_time(start_str)
        end_sec = parse_srt_time(end_str)

        if start_sec < prev_end:
            print(f"âš ï¸ é‡è¤‡: #{idx+1} start={start_sec:.3f}s overlaps with previous end={prev_end:.3f}s")
        prev_end = end_sec

    print("âœ… å­—å¹•ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãƒã‚§ãƒƒã‚¯å®Œäº†\n")

def make_offset_func(direction, offset, duration):
    if direction == "up":
        return lambda t: 0, lambda t: offset * (1 - 2 * t / duration)
    elif direction == "down":
        return lambda t: 0, lambda t: -offset * (1 - 2 * t / duration)
    elif direction == "left":
        return lambda t: offset * (1 - 2 * t / duration), lambda t: 0
    elif direction == "right":
        return lambda t: -offset * (1 - 2 * t / duration), lambda t: 0
    elif direction == "diag":
        return (
            lambda t: -offset * (1 - 2 * t / duration),
            lambda t: -offset * (1 - 2 * t / duration),
        )
    else:
        return lambda t: 0, lambda t: 0


def make_cropper(x_offset_func, y_offset_func, scale_func=None, duration=1.0):
    def fl(gf, t):
        t = min(t, duration)
        frame = gf(t)
        zoom = scale_func(t) if scale_func else 1.0
        crop_half = int(360 / zoom)

        x_center = 512 + int(x_offset_func(t))
        y_center = 512 + int(y_offset_func(t))

        return frame[
            y_center - crop_half:y_center + crop_half,
            x_center - crop_half:x_center + crop_half
        ]
    return fl



def compose_video(script_id: str):
    timing_path = Path(f"data/stage_2_tag/tags_{script_id}.json")
    subtitle_path = Path(f"data/stage_4_subtitles/subtitles_{script_id}.srt")
    audio_base_dir = Path(f"data/stage_1_audio/{script_id}")
    image_base_dir = Path(f"data/stage_5_image/{script_id}")
    output_dir = Path(f"data/stage_6_output/{script_id}")
    output_dir.mkdir(parents=True, exist_ok=True)

    with open(timing_path, "r", encoding="utf-8") as f:
        data = json.load(f)
        scenes = data["scenes"]

    clips = []
    audio_clips = []
    current_start = 0.0  # ç´¯ç©å‹ã®å†ç”Ÿé–‹å§‹æ™‚åˆ»ï¼ˆã™ã¹ã¦ã®åŸºæº–ï¼‰

    # âœ… parent_scene_id ã”ã¨ã«èƒŒæ™¯ç”»åƒã‚’è¡¨ç¤º
    from collections import defaultdict

    # âœ… parent_scene_id ã”ã¨ã«èƒŒæ™¯ç”»åƒã‚’è¡¨ç¤ºï¼ˆéŸ³å£°ã¨å®Œå…¨åŒæœŸã•ã›ã‚‹ï¼‰
    parent_scene_map = defaultdict(list)
    for scene in scenes:
        parent_scene_map[scene["parent_scene_id"]].append(scene)

    timeline_pointer = 0.0  # ç”»åƒè¡¨ç¤ºã®ç´¯ç©é–‹å§‹æ™‚é–“ã€‚éŸ³å£°ã¨åŒã˜ãå‰ã‹ã‚‰é †ã«ç©ã¿ä¸Šã’ã¦ã„ã

    for parent_id, group in parent_scene_map.items():
        video_path_mv = image_base_dir / f"{parent_id}_mv.mp4"
        video_path = image_base_dir / f"{parent_id}.mp4"
        img_path_mv = image_base_dir / f"{parent_id}_mv.png"
        img_path_default = image_base_dir / f"{parent_id}.png"

        # å„ªå…ˆé †ä½: _mv.mp4 â†’ .mp4 â†’ _mv.png â†’ .png
        if video_path_mv.exists():
            asset_path = video_path_mv
            asset_type = "video"
        elif video_path.exists():
            asset_path = video_path
            asset_type = "video"
        elif img_path_mv.exists():
            asset_path = img_path_mv
            asset_type = "image"
        elif img_path_default.exists():
            asset_path = img_path_default
            asset_type = "image"
        else:
            print(f"âš ï¸ ç´ æãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {parent_id}")
            continue


        # âœ… ãã®ã‚°ãƒ«ãƒ¼ãƒ—ã«å«ã¾ã‚Œã‚‹å„ scene_id.wav ã®å†ç”Ÿæ™‚é–“ï¼ˆï¼‹SILENCEï¼‰ã‚’ç©ç®—
        group_duration = 0.0
        for scene in group:
            audio_path = audio_base_dir / f"{scene['scene_id']}.wav"
            if not audio_path.exists():
                continue
            audio_clip = AudioFileClip(str(audio_path))
            group_duration += audio_clip.duration + SILENCE_DURATION

        # âœ… ã“ã®è¦ªsceneã®ç”»åƒã‚’è¡¨ç¤ºã™ã‚‹é–‹å§‹æ™‚åˆ»ã¨è¡¨ç¤ºæ™‚é–“ã‚’æ±ºå®šï¼ˆï¼éŸ³å£°ã¨å®Œå…¨ä¸€è‡´ï¼‰
        start_time = timeline_pointer
        duration = group_duration
        end_time = start_time + duration
        timeline_pointer = end_time  # æ¬¡ã®è¦ªsceneã®ç”»åƒè¡¨ç¤ºé–‹å§‹æ™‚é–“ã«æ›´æ–°

        if asset_type == "image":
            pil_image = Image.open(asset_path).convert("RGB")
            np_frame = np.array(pil_image)
            img_clip = ImageClip(np_frame)

            offset = 50
            x_offset_func = lambda t: 0
            y_offset_func = lambda t: 0
            scale_func = None

            if asset_path.name.endswith("_mv.png"):
                effect = random.choice([
                    "pan_up", "pan_down", "pan_left", "pan_right", "pan_diag",
                    "zoom_in", "zoom_out"
                ])
                print(f"âœ¨ effect applied to {parent_id}: {effect}")

                if effect == "pan_up":
                    x_offset_func, y_offset_func = make_offset_func("up", offset, duration)
                elif effect == "pan_down":
                    x_offset_func, y_offset_func = make_offset_func("down", offset, duration)
                elif effect == "pan_left":
                    x_offset_func, y_offset_func = make_offset_func("left", offset, duration)
                elif effect == "pan_right":
                    x_offset_func, y_offset_func = make_offset_func("right", offset, duration)
                elif effect == "pan_diag":
                    x_offset_func, y_offset_func = make_offset_func("diag", offset, duration)
                elif effect == "zoom_in":
                    scale_func = lambda t: 1.0 + 0.1 * (t / duration)
                elif effect == "zoom_out":
                    scale_func = lambda t: 1.1 - 0.1 * (t / duration)

                img_clip = img_clip.fl(
                    make_cropper(x_offset_func, y_offset_func, scale_func, duration),
                    apply_to=["mask"]
                )

            img_clip = img_clip.set_position(("center", "center")).resize((720, 720))
            clip = img_clip.set_start(start_time).set_duration(duration).set_fps(30)

            print(f"ğŸ–¼ï¸ ç”»åƒclip: parent_id={parent_id}, start={start_time:.2f}s, duration={duration:.2f}s, file={asset_path.name}")

        elif asset_type == "video":
            clip = (
                VideoFileClip(str(asset_path))
                .without_audio()
                .set_start(start_time)
                .set_duration(duration)
                .set_fps(30)
            )

            # âœ… ä¸­å¤®æ­£æ–¹å½¢720Ã—720ã§cropã™ã‚‹ã ã‘ï¼ˆãƒ‘ãƒ³ãƒ»ã‚ºãƒ¼ãƒ ãªã—ï¼‰
            clip = clip.crop(x_center=clip.w // 2, y_center=clip.h // 2, width=720, height=720)
            clip = clip.set_position(("center", "center"))

        clips.append(clip)  # â† ã“ã“

    for i, scene in enumerate(scenes):
        scene_id = scene["scene_id"]
        audio_path = audio_base_dir / f"{scene_id}.wav"

        if not audio_path.exists():
            print(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {scene_id}ï¼ˆç”»åƒã¾ãŸã¯éŸ³å£°ãŒè¦‹ã¤ã‹ã‚‰ãªã„ï¼‰")
            continue

        # éŸ³å£°èª­ã¿è¾¼ã¿ + ç„¡éŸ³0ç§’è¿½åŠ 
        audio_clip = AudioFileClip(str(audio_path))
        silence = AudioClip(make_frame=lambda t: [0], duration=SILENCE_DURATION, fps=44100).set_fps(44100)
        audio_clip = concatenate_audioclips([audio_clip, silence])
        real_duration = audio_clip.duration  # SILENCE_DURATIONã‚’å«ã‚€

        # å®‰å…¨ãªæ™‚é–“å®šç¾©
        start_time = current_start
        end_time = start_time + real_duration
        duration = end_time - start_time  # â† duration ã‚’å…ˆã«å®šç¾©ï¼


        # éŸ³å£°clipï¼ˆæ˜ç¤ºçš„ã«start/endã‚’æŒ‡å®šï¼‰
        audio_clip = audio_clip.set_start(start_time).set_end(end_time)
        audio_clips.append(audio_clip)

        current_start = end_time  # æ¬¡ã®sceneã®åŸºæº–æ™‚é–“ã«é€²ã‚ã‚‹

    if not clips:
        print("âŒ æœ‰åŠ¹ãªsceneãŒã‚ã‚Šã¾ã›ã‚“ã€‚å‹•ç”»ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚")
        return

    final_audio = concatenate_audioclips(audio_clips)

    # ======== ğŸµ BGMãƒ»åŠ¹æœéŸ³ã®æŒ¿å…¥ ========

    project_root = Path(__file__).parent.parent
    fixed_assets_dir = project_root / "fixed_assets"

    bgm_path = fixed_assets_dir / "bgm.mp3"
    se_main_path = fixed_assets_dir / "se_main_title.mp3"
    se_center_path = fixed_assets_dir / "se_title_center.mp3"

    print(f"ğŸ” BGMãƒ‘ã‚¹ï¼š{bgm_path}")
    print(f"å­˜åœ¨ã™ã‚‹ã‹ï¼Ÿ {bgm_path.exists()}")

    # BGMï¼ˆå…¨ä½“ã«ãƒ«ãƒ¼ãƒ—ï¼‰
    if not bgm_path.exists():
        print("âš ï¸ BGMãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        bgm_clip = None
    else:
        bgm_clip = AudioFileClip(str(bgm_path))
        bgm_duration = final_audio.duration
        bgm_loop = afx.audio_loop(bgm_clip, duration=bgm_duration).volumex(0.02)  # éŸ³é‡èª¿æ•´ï¼ˆ0.0ã€œ1.0ï¼‰

    subtitle_json_path = Path(f"data/stage_4_subtitles/subtitles_{script_id}.json")
    with open(subtitle_json_path, "r", encoding="utf-8") as f:
        subtitle_scenes = json.load(f)


    # åŠ¹æœéŸ³ã®æŒ¿å…¥ï¼ˆmain_title: ãƒ‰ãƒ‰ãƒ³, title_top: ãƒ‰ãƒ³ï¼‰
    se_clips = []

    for scene in subtitle_scenes:
        scene_type = scene.get("type", "")
        scene_start = scene.get("start_sec", None)
        if scene_start is None:
            continue

        if scene_type == "main_title_top" and se_main_path.exists():
            se_clip = AudioFileClip(str(se_main_path)).set_start(scene_start).volumex(0.5)
            se_clips.append(se_clip)
        elif scene_type == "title_center" and se_center_path.exists():
            se_clip = AudioFileClip(str(se_center_path)).set_start(scene_start).volumex(0.4)
            se_clips.append(se_clip)

    # ======== ğŸ§ éŸ³å£°åˆæˆ ========
    audio_layers = [final_audio]  # ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸»ä½“
    if bgm_clip:
        audio_layers.append(bgm_loop)
    audio_layers.extend(se_clips)

    composite_audio = CompositeAudioClip(audio_layers)

    # âœ… base_video ã‚’ã“ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§å®šç¾©
    base_video = CompositeVideoClip(clips, size=(VIDEO_WIDTH, VIDEO_HEIGHT))

    # âœ… éŸ³å£°ã‚’åˆæˆ
    final = base_video.set_audio(composite_audio)

    temp_path = output_dir / "no_subtitles.mp4"
    export_video_high_quality(final, str(temp_path))

        # .asså­—å¹•ã‚’ä½¿ã£ã¦ no_subtitles.mp4 â†’ final.mp4 ã‚’ç”Ÿæˆ
    ass_path = Path(f"data/stage_4_subtitles/subtitles_{script_id}.ass")
    final_path = output_dir / "final.mp4"

    if not ass_path.exists():
        raise FileNotFoundError(f"âŒ .ass å­—å¹•ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ass_path}")

    ffmpeg_ass_cmd = (
        f'ffmpeg -y -hwaccel cuda -i "{temp_path}" '
        f'-vf "ass={ass_path.as_posix()}" '
        f'-c:v h264_nvenc -preset slow -b:v 4000k -maxrate 4000k -bufsize 8000k '
        f'-pix_fmt yuv420p -profile:v baseline -level 4.0 '
        f'-c:a aac -b:a 128k -ar 44100 -ac 2 '
        f'"{final_path}"'
    )

    print(f"[ASSå­—å¹•ç„¼ãè¾¼ã¿] {ffmpeg_ass_cmd}")
    ret = os.system(ffmpeg_ass_cmd)
    if ret != 0:
        raise RuntimeError(f"âŒ .asså­—å¹•ç„¼ãè¾¼ã¿å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆcode={ret}ï¼‰")
    print(f"âœ… .asså­—å¹•ä»˜ãå‹•ç”»ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {final_path}")


    # if subtitle_path.exists():
    #     check_srt_overlaps(subtitle_path)

    #     ffmpeg_command = (
    #         f'ffmpeg -y -hwaccel cuda -i "{temp_path}" '
    #         f'-vf "subtitles=\'{subtitle_path.as_posix()}\':force_style='
    #         f'\'Fontname=Noto Sans CJK JP, Alignment=2,Fontsize=18,Outline=2,MarginV=80,Shadow=1, '
    #         f'PrimaryColour=&H00FFFFFF,OutlineColour=&H00000000,ShadowColour=&H80000000,BorderStyle=1, LineSpacing=0\'" '
    #         f'-c:v h264_nvenc -preset slow -b:v 4000k -maxrate 4000k -bufsize 8000k '
    #         f'-pix_fmt yuv420p -profile:v baseline -level 4.0 '
    #         f'-c:a aac -b:a 128k -ar 44100 -ac 2 '
    #         f'"{final_path}"'
    #     )
    #     print(f"[GPUå­—å¹•ç„¼ãè¾¼ã¿] {ffmpeg_command}")
    #     ret = os.system(ffmpeg_command)
    #     if ret != 0:
    #         raise RuntimeError(f"âŒ å­—å¹•ç„¼ãè¾¼ã¿ffmpegå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆcode={ret}ï¼‰")
    #     print(f"âœ… å­—å¹•ä»˜ãå‹•ç”»ã‚’GPUã§ä¿å­˜ã—ã¾ã—ãŸ: {final_path}")

    # else:
    #     print(f"âš ï¸ å­—å¹•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å­—å¹•ãªã—ã§ä¿å­˜: {temp_path}")
    #     temp_path.rename(final_path)

    compatible_path = output_dir / "final_compatible.mp4"
    ffmpeg_compat_cmd = (
        f'ffmpeg -y -hwaccel cuda -i "{final_path}" '
        f'-vf "scale=trunc(iw/2)*2:trunc(ih/2)*2,format=yuv420p" '
        f'-c:v h264_nvenc -preset slow -b:v 4000k -maxrate 4000k -bufsize 8000k -pix_fmt yuv420p '
        f'-profile:v baseline -level 4.0 '
        f'-c:a aac -b:a 128k -ar 44100 -ac 2 '
        f'"{compatible_path}"'
    )


    print(f"[GPUäº’æ›å¤‰æ›] {ffmpeg_compat_cmd}")
    os.system(ffmpeg_compat_cmd)
    print(f"âœ… å†ç”Ÿäº’æ›ç‰ˆå‹•ç”»ï¼ˆGPUä½¿ç”¨ï¼‰ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {compatible_path}")


if __name__ == "__main__":
    task_name = "compose"

    while True:
        script_id = get_next_script_id(task_name)
        if not script_id:
            print("âœ… å…¨ã¦ã® script_id ã«å¯¾ã—ã¦ compose ãŒå®Œäº†ã—ã¦ã„ã¾ã™ã€‚")
            break

        print(f"ğŸ¬ å‡¦ç†å¯¾è±¡ã®script_id: {script_id}")
        try:
            compose_video(script_id)
            mark_script_completed(script_id, task_name)
            print(f"âœ… å®Œäº†: {script_id}")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {script_id} ã§ä¾‹å¤–ç™ºç”Ÿ: {e}")


